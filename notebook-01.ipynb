{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello this is my first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world~\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello world~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU pymongo datasets langchain fireworks-ai tiktoken sentence_transformers tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Setup prerequisites\n",
    "Replace:\n",
    "\n",
    "<MONGODB_URI> with your MongoDB connection string\n",
    "<FIREWORKS_API_KEY> with your Fireworks API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ok': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retain the quotes (\"\") when pasting the URI\n",
    "MONGODB_URI = \"mongodb+srv://chinsiang99:Fy96flbT4pSY6i4z@cluster0.awgdv.mongodb.net/\"\n",
    "# Initialize a MongoDB Python client\n",
    "mongodb_client = MongoClient(MONGODB_URI, appname=\"devrel.workshop.rag\")\n",
    "# Check the connection to the server\n",
    "mongodb_client.admin.command(\"ping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the quotes (\"\") when pasting the API key\n",
    "os.environ[\"FIREWORKS_API_KEY\"] = \"EHFoghFVxhgAgBKJGbtTN5mh4ueyME7NToJIVzmuf9Jewksj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"mongodb/devcenter-articles\", split=\"train\", streaming=True)\n",
    "data_head = data.take(20)\n",
    "docs = pd.DataFrame(data_head).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of documents in the dataset\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sourceName': 'devcenter',\n",
       " 'url': 'https://www.mongodb.com/developer/products/atlas/atlas-search-cene-1',\n",
       " 'action': 'created',\n",
       " 'body': '# The Atlas Search \\'cene: Season 1\\n\\n# The Atlas Search \\'cene: Season 1\\n\\nWelcome to the first season of a video series dedicated to Atlas Search!  This series of videos is designed to guide you through the journey from getting started and understanding the concepts, to advanced techniques.\\n\\n## What is Atlas Search?\\n\\n[Atlas Search][1] is an embedded full-text search in MongoDB Atlas that gives you a seamless, scalable experience for building relevance-based app features. Built on Apache Lucene, Atlas Search eliminates the need to run a separate search system alongside your database.\\n\\nBy integrating the database, search engine, and sync mechanism into a single, unified, and fully managed platform, Atlas Search is the fastest and easiest way to build relevance-based search capabilities directly into applications.\\n\\n> Hip to the *\\'cene*\\n> \\n> The name of this video series comes from a contraction of \"Lucene\",\\n> the search engine library leveraged by Atlas. Or it\\'s a short form of \"scene\". \\n\\n## Episode Guide\\n\\n### **[Episode 1: What is Atlas Search & Quick Start][2]**\\n\\nIn this first episode of the Atlas Search \\'cene, learn what Atlas Search is, and get a quick start introduction to setting up Atlas Search on your data.  Within a few clicks, you can set up a powerful, full-text search index on your Atlas collection data, and leverage the fast, relevant results to your users queries.\\n\\n### **[Episode 2: Configuration / Development Environment][3]**\\n\\nIn order to best leverage Atlas Search, configuring it for your querying needs leads to success. In this episode, learn how Atlas Search maps your documents to its index, and discover the configuration control you have.\\n\\n### **[Episode 3: Indexing][4]**\\n\\nWhile Atlas Search automatically indexes your collections content, it does demand attention to the indexing configuration details in order to match users queries appropriately. This episode covers how Atlas Search builds an inverted index, and the options one must consider.\\n\\n### **[Episode 4: Searching][5]**\\n\\nAtlas Search provides a rich set of query operators and relevancy controls. This episode covers the common query operators, their relevancy controls, and ends with coverage of the must-have Query Analytics feature.\\n\\n### **[Episode 5:  Faceting][6]**\\n\\nFacets produce additional context for search results, providing a list of subsets and counts within. This episode details the faceting options available in Atlas Search.\\n\\n### **[Episode 6: Advanced Search Topics][7]**\\n\\nIn this episode, we go through some more advanced search topics including embedded documents, fuzzy search, autocomplete, highlighting, and geospatial.\\n\\n### **[Episode 7:  Query Analytics][8]**\\n\\nAre your users finding what they are looking for? Are your top queries returning the best results? This episode covers the important topic of query analytics. If you\\'re using search, you need this!\\n\\n### **[Episode 8:  Tips & Tricks][9]**\\n\\nIn this final episode of The Atlas Search \\'cene Season 1, useful techniques to introspect query details and see the relevancy scoring computation details. Also shown is how to get facets and search results back in one API call.\\n\\n  [1]: https://www.mongodb.com/atlas/search\\n  [2]: https://www.mongodb.com/developer/videos/what-is-atlas-search-quick-start/\\n  [3]: https://www.mongodb.com/developer/videos/atlas-search-configuration-development-environment/\\n  [4]: https://www.mongodb.com/developer/videos/mastering-indexing-for-perfect-query-matches/\\n  [5]: https://www.mongodb.com/developer/videos/query-operators-relevancy-controls-for-precision-searches/\\n  [6]: https://www.mongodb.com/developer/videos/faceting-mastery-unlock-the-full-potential-of-atlas-search-s-contextual-insights/\\n  [7]: https://www.mongodb.com/developer/videos/atlas-search-mastery-elevate-your-search-with-fuzzy-geospatial-highlighting-hacks/\\n  [8]: https://www.mongodb.com/developer/videos/atlas-search-query-analytics/\\n  [9]: https://www.mongodb.com/developer/videos/tips-and-tricks-the-atlas-search-cene-season-1-episode-8/',\n",
       " 'format': 'md',\n",
       " 'metadata': {'tags': ['Atlas'],\n",
       "  'pageDescription': \"The Atlas Search 'cene: Season 1\",\n",
       "  'contentType': 'Video'},\n",
       " 'title': \"The Atlas Search 'cene: Season 1\",\n",
       " 'updated': '2024-05-20T17:32:23.500Z'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview a document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Chunk up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separators to split on\n",
    "separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/split_by_token/#tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `RecursiveCharacterTextSplitter` text splitter with the `cl100k_base` encoding\n",
    "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk\n",
    "# Chunk overlap of 15-20% of the chunk size is recommended\n",
    "# Pass the `separators` list above as an argument called `separators`\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", separators=separators, chunk_size=200, chunk_overlap=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunk up a document.\n",
    "\n",
    "    Args:\n",
    "        doc (Dict): Parent document to generate chunks from.\n",
    "        text_field (str): Text field to chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of chunked documents.\n",
    "    \"\"\"\n",
    "    # Extract the field to chunk from `doc`\n",
    "    text = doc[text_field]\n",
    "    # Split `text` using the appropriate method of the `RecursiveCharacterTextSplitter` class\n",
    "    # NOTE: `text` is a string\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    # Iterate through `chunks` and for each chunk:\n",
    "    # 1. Create a shallow copy of `doc`, call it `temp`\n",
    "    # 2. Set the `text_field` field in `temp` to the content of the chunk\n",
    "    # 3. Append `temp` to `chunked_data`\n",
    "    chunked_data = []\n",
    "    for chunk in chunks:\n",
    "       temp = doc.copy()\n",
    "       temp[text_field]=chunk\n",
    "       chunked_data.append(temp)\n",
    "\n",
    "    return chunked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through `docs`, use the `get_chunks` function to chunk up the documents based on the \"body\" field, and add the list of chunked documents to `split_docs` initialized above.\n",
    "split_docs = []\n",
    "for doc in docs:\n",
    "    chunks = get_chunks(doc, \"body\")\n",
    "    split_docs.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the length of the list of chunked documents is greater than the length of `docs`\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sourceName': 'devcenter',\n",
       " 'url': 'https://www.mongodb.com/developer/products/atlas/atlas-search-cene-1',\n",
       " 'action': 'created',\n",
       " 'body': \"# The Atlas Search 'cene: Season 1\\n\\n# The Atlas Search 'cene: Season 1\\n\\nWelcome to the first season of a video series dedicated to Atlas Search!  This series of videos is designed to guide you through the journey from getting started and understanding the concepts, to advanced techniques.\\n\\n## What is Atlas Search?\\n\\n[Atlas Search][1] is an embedded full-text search in MongoDB Atlas that gives you a seamless, scalable experience for building relevance-based app features. Built on Apache Lucene, Atlas Search eliminates the need to run a separate search system alongside your database.\\n\\nBy integrating the database, search engine, and sync mechanism into a single, unified, and fully managed platform, Atlas Search is the fastest and easiest way to build relevance-based search capabilities directly into applications.\",\n",
       " 'format': 'md',\n",
       " 'metadata': {'tags': ['Atlas'],\n",
       "  'pageDescription': \"The Atlas Search 'cene: Season 1\",\n",
       "  'contentType': 'Video'},\n",
       " 'title': \"The Atlas Search 'cene: Season 1\",\n",
       " 'updated': '2024-05-20T17:32:23.500Z'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview one of the items in split_docs- ensure that it is a Python dictionary\n",
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the `gte-small` model using the Sentence Transformers library\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://huggingface.co/thenlper/gte-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
    "# An array can be converted to a list using the `tolist()` method\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate the embedding for a piece of text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to embed.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Embedding of the text as a list.\n",
    "    \"\"\"\n",
    "    embedding = embedding_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 218/218 [00:32<00:00,  6.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add an `embedding` field to each dictionary in `split_docs`\n",
    "# The `embedding` field should correspond to the embedding of the value of the `body` field\n",
    "# Use the `get_embedding` function defined above to generate the embedding\n",
    "# Append the updated dictionaries to `embedded_docs` initialized above.\n",
    "for doc in tqdm(split_docs):\n",
    "    doc[\"embedding\"] = get_embedding(doc[\"body\"])\n",
    "    embedded_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the length of `embedded_docs` is the same as that of `split_docs`\n",
    "len(embedded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Ingest data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"mongodb_rag_lab\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-database\n",
    "\n",
    "📚 https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the collection defined above using the `mongodb_client` defined in Step 2\n",
    "collection = mongodb_client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 218, 'electionId': ObjectId('7fffffff0000000000000617'), 'opTime': {'ts': Timestamp(1724576791, 251), 't': 1559}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1724576791, 259), 'signature': {'hash': b'\\x82\\x12}\\xf4\\x15\\xb2X\\x14w\\xd5\\xc2-\\x18\\xaa\\x83\\xf1\\xcf\\x16\\x049', 'keyId': 7365108054888546319}}, 'operationTime': Timestamp(1724576791, 251)}, acknowledged=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulk delete all existing records from the collection defined above\n",
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/examples/bulk.html#bulk-insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "collection.insert_many(embedded_docs) \n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Create a vector search index\n",
    "Follow the instructions in the documentation to create a Vector Search index in the Atlas UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Perform semantic search on your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a vector search function\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#fields\n",
    "\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Basic Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve relevant documents for a user query using vector search\n",
    "def vector_search(user_query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for a user query using vector search.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 5\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
    "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
    "    # In the $project stage, exclude the `_id` field and include only the `body` field and `vectorSearchScore`\n",
    "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
    "    pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"path\": \"embedding\",\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "    # Execute the aggregation `pipeline` and store the results in `results`\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run vector search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': \"# The Atlas Search 'cene: Season 1\\n\\n# The Atlas Search 'cene: Season 1\\n\\nWelcome to the first season of a video series dedicated to Atlas Search!  This series of videos is designed to guide you through the journey from getting started and understanding the concepts, to advanced techniques.\\n\\n## What is Atlas Search?\\n\\n[Atlas Search][1] is an embedded full-text search in MongoDB Atlas that gives you a seamless, scalable experience for building relevance-based app features. Built on Apache Lucene, Atlas Search eliminates the need to run a separate search system alongside your database.\\n\\nBy integrating the database, search engine, and sync mechanism into a single, unified, and fully managed platform, Atlas Search is the fastest and easiest way to build relevance-based search capabilities directly into applications.\",\n",
       "  'score': 0.9609129428863525},\n",
       " {'body': '[1]: https://www.mongodb.com/atlas/search\\n  [2]: https://www.mongodb.com/developer/videos/what-is-atlas-search-quick-start/\\n  [3]: https://www.mongodb.com/developer/videos/atlas-search-configuration-development-environment/\\n  [4]: https://www.mongodb.com/developer/videos/mastering-indexing-for-perfect-query-matches/\\n  [5]: https://www.mongodb.com/developer/videos/query-operators-relevancy-controls-for-precision-searches/\\n  [6]: https://www.mongodb.com/developer/videos/faceting-mastery-unlock-the-full-potential-of-atlas-search-s-contextual-insights/\\n  [7]: https://www.mongodb.com/developer/videos/atlas-search-mastery-elevate-your-search-with-fuzzy-geospatial-highlighting-hacks/\\n  [8]: https://www.mongodb.com/developer/videos/atlas-search-query-analytics/',\n",
       "  'score': 0.9582604765892029},\n",
       " {'body': '# How to Model Your Documents for Vector Search\\n\\nAtlas Vector Search was recently released, so let’s dive into a tutorial on how to properly model your documents when utilizing vector search to revolutionize your querying capabilities!\\n\\n## Data modeling normally in MongoDB\\n\\nVector search is new, so let’s first go over the basic ways of modeling your data in a MongoDB document before continuing on into how to incorporate vector embeddings. \\n\\nData modeling in MongoDB revolves around organizing your data into documents within various collections. Varied projects or organizations will require different ways of structuring data models due to the fact that successful data modeling depends on the specific requirements of each application, and for the most part, no one document design can be applied for every situation. There are some commonalities, though, that can guide the user. These are:',\n",
       "  'score': 0.9525887370109558},\n",
       " {'body': '[8]: https://www.mongodb.com/developer/videos/atlas-search-query-analytics/\\n  [9]: https://www.mongodb.com/developer/videos/tips-and-tricks-the-atlas-search-cene-season-1-episode-8/',\n",
       "  'score': 0.9513592720031738},\n",
       " {'body': '# How to Deploy MongoDB Atlas with AWS CloudFormation\\n\\nMongoDB Atlas is the multi-cloud developer data platform that provides an integrated suite of cloud database and data services. We help to accelerate and simplify how you build resilient and performant global applications on the cloud provider of your choice.\\n\\nAWS CloudFormation lets you model, provision, and manage AWS and third-party resources like MongoDB Atlas by treating infrastructure as code (IaC). CloudFormation templates are written in either JSON or YAML. \\n\\nWhile there are multiple ways to use CloudFormation to provision and manage your Atlas clusters, such as with Partner Solution Deployments or the AWS CDK, today we’re going to go over how to create your first YAML CloudFormation templates to deploy Atlas clusters with CloudFormation.',\n",
       "  'score': 0.9507526159286499}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"What is MongoDB Atlas Search?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '# How to Send MongoDB Document Changes to a Slack Channel\\n\\nIn this tutorial, we will explore a seamless integration of your database with Slack using Atlas Triggers and the Slack API. Discover how to effortlessly send notifications to your desired Slack channels, effectively connecting the operations happening within your collections and relaying them in real-time updates. \\n\\nThe overall flow will be: \\n\\n.\\n\\nOnce this has been completed, we are ready to start creating our first database trigger that will react every time there is an operation in a certain collection. \\n\\n## Atlas trigger\\n\\nFor this tutorial, we will create a trigger that monitors all changes in a `test` collection for `insert`, `update`, and `delete` operations.\\n\\nTo create a new database trigger, you will need to:',\n",
       "  'score': 0.9634033441543579},\n",
       " {'body': \"To create a new database trigger, you will need to:\\n\\n1. Click the **Data Services** tab in the top navigation of your screen if you haven't already navigated to Atlas.\\n2. Click **Triggers** in the left-hand navigation.\\n3. On the **Overview** tab of the **Triggers** page, click **Add Trigger** to open the trigger configuration page.\\n4. Enter the configuration values for the trigger and click **Save** at the bottom of the page.\\n\\nPlease note that this trigger will make use of the *event ordering* as we want the operations to be processed according to when they were performed. \\n\\nThe trigger configuration values will look like this: \\n\\n using the UI, we need to:   \\n\\n1. Click the **Data Services** tab in the top navigation of your screen if you haven't already navigated to Atlas.\\n\\n2. Click **Functions** in the left navigation menu.\",\n",
       "  'score': 0.9527696371078491},\n",
       " {'body': '## Conclusion\\n\\nI hope you like my template, and I hope I will help you be more productive with MongoDB and the Java stack.\\n\\nIf you see something which can be improved, please feel free to open a GitHub issue or directly submit a pull request. They are very welcome. :-)\\n\\nIf you are new to MongoDB Atlas, give our Quick Start post a try to get up to speed with MongoDB Atlas in no time.\\n\\n  [1]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt876f3404c57aa244/65388189377588ba166497b0/swaggerui.png\\n  [2]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf2f06ba5af19464d/65388188d31953242b0dbc6f/nyancat.png',\n",
       "  'score': 0.9403784275054932},\n",
       " {'body': 'to only detect certain changes and then adapt the change event to only receive certain fields with a \"$project\".\\n\\n## Conclusion\\n\\nIn this tutorial, we\\'ve learned how to seamlessly integrate your database with Slack using Atlas Triggers and the Slack API. This integration allows you to send real-time notifications to your Slack channels, keeping your team informed about important operations within your database collections.\\n\\nWe started by creating a new application in Atlas and then set up a database trigger that reacts to specific collection operations. We explored the `processEvent` function, which processes change events and prepares the data for Slack notifications. Through a step-by-step process, we demonstrated how to create a message and use the Slack API to post it to a specific channel.\\n\\nNow that you\\'ve grasped the basics, it\\'s time to take your integration skills to the next level. Here are some steps you can follow:',\n",
       "  'score': 0.939049482345581},\n",
       " {'body': 'These pre-made templates directly leverage MongoDB Atlas resources from the CloudFormation Public Registry and execute via the AWS CLI/AWS Management Console. Using these is best for users who seek to be tightly integrated into AWS with fine-grained access controls. \\n\\nLet’s get started! \\n\\n*Prerequisites:* \\n\\n- Install and configure an AWS Account and the AWS CLI.\\n- Install and configure the MongoDB Atlas CLI (optional but recommended). \\n\\n## Step 1: Create a MongoDB Atlas account\\n\\nSign up for a free MongoDB Atlas account, verify your email address, and log into your new account.\\n\\nAlready have an AWS account? Atlas supports paying for usage via the AWS Marketplace (AWS MP) without any upfront commitment — simply sign up for MongoDB Atlas via AWS Marketplace.\\n\\nand contact AWS support directly, who can help confirm the CIDR range to be used in your Atlas PAK IP Whitelist. \\n\\n on MongoDB Atlas.',\n",
       "  'score': 0.9387099742889404}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"What are triggers in MongoDB Atlas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🦹‍♀️ Combine pre-filtering with vector search\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#about-the-filter-type\n",
    "\n",
    "📚 https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Filter Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for documents where the content type is Video\n",
    "Modify the vector search index definition (from Step 7) in the Atlas UI to include the metadata.contentType field as a filter field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user query\n",
    "query_embedding = get_embedding(\"What is MongoDB Atlas Search?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where the `metadata.contentType` field has the value \"Video\"\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "            \"filter\": {\"metadata.contentType\": \"Video\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': \"# The Atlas Search 'cene: Season 1\\n\\n# The Atlas Search 'cene: Season 1\\n\\nWelcome to the first season of a video series dedicated to Atlas Search!  This series of videos is designed to guide you through the journey from getting started and understanding the concepts, to advanced techniques.\\n\\n## What is Atlas Search?\\n\\n[Atlas Search][1] is an embedded full-text search in MongoDB Atlas that gives you a seamless, scalable experience for building relevance-based app features. Built on Apache Lucene, Atlas Search eliminates the need to run a separate search system alongside your database.\\n\\nBy integrating the database, search engine, and sync mechanism into a single, unified, and fully managed platform, Atlas Search is the fastest and easiest way to build relevance-based search capabilities directly into applications.\",\n",
       "  'score': 0.9609129428863525},\n",
       " {'body': '[1]: https://www.mongodb.com/atlas/search\\n  [2]: https://www.mongodb.com/developer/videos/what-is-atlas-search-quick-start/\\n  [3]: https://www.mongodb.com/developer/videos/atlas-search-configuration-development-environment/\\n  [4]: https://www.mongodb.com/developer/videos/mastering-indexing-for-perfect-query-matches/\\n  [5]: https://www.mongodb.com/developer/videos/query-operators-relevancy-controls-for-precision-searches/\\n  [6]: https://www.mongodb.com/developer/videos/faceting-mastery-unlock-the-full-potential-of-atlas-search-s-contextual-insights/\\n  [7]: https://www.mongodb.com/developer/videos/atlas-search-mastery-elevate-your-search-with-fuzzy-geospatial-highlighting-hacks/\\n  [8]: https://www.mongodb.com/developer/videos/atlas-search-query-analytics/',\n",
       "  'score': 0.9582604765892029},\n",
       " {'body': '[8]: https://www.mongodb.com/developer/videos/atlas-search-query-analytics/\\n  [9]: https://www.mongodb.com/developer/videos/tips-and-tricks-the-atlas-search-cene-season-1-episode-8/',\n",
       "  'score': 0.9513592720031738},\n",
       " {'body': '### **[Episode 3: Indexing][4]**\\n\\nWhile Atlas Search automatically indexes your collections content, it does demand attention to the indexing configuration details in order to match users queries appropriately. This episode covers how Atlas Search builds an inverted index, and the options one must consider.\\n\\n### **[Episode 4: Searching][5]**\\n\\nAtlas Search provides a rich set of query operators and relevancy controls. This episode covers the common query operators, their relevancy controls, and ends with coverage of the must-have Query Analytics feature.\\n\\n### **[Episode 5:  Faceting][6]**\\n\\nFacets produce additional context for search results, providing a list of subsets and counts within. This episode details the faceting options available in Atlas Search.\\n\\n### **[Episode 6: Advanced Search Topics][7]**\\n\\nIn this episode, we go through some more advanced search topics including embedded documents, fuzzy search, autocomplete, highlighting, and geospatial.',\n",
       "  'score': 0.9452466368675232},\n",
       " {'body': '> Hip to the *\\'cene*\\n> \\n> The name of this video series comes from a contraction of \"Lucene\",\\n> the search engine library leveraged by Atlas. Or it\\'s a short form of \"scene\". \\n\\n## Episode Guide\\n\\n### **[Episode 1: What is Atlas Search & Quick Start][2]**\\n\\nIn this first episode of the Atlas Search \\'cene, learn what Atlas Search is, and get a quick start introduction to setting up Atlas Search on your data.  Within a few clicks, you can set up a powerful, full-text search index on your Atlas collection data, and leverage the fast, relevant results to your users queries.\\n\\n### **[Episode 2: Configuration / Development Environment][3]**\\n\\nIn order to best leverage Atlas Search, configuring it for your querying needs leads to success. In this episode, learn how Atlas Search maps your documents to its index, and discover the configuration control you have.',\n",
       "  'score': 0.9419649839401245}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the aggregation pipeline and view the results\n",
    "results = collection.aggregate(pipeline)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter on documents which have been updated on or after 2024-05-19 and where the content type is Tutorial\n",
    "Modify the index definition (from Step 7) in the Atlas UI to include the metadata.contentType and updated fields as filter fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the user query\n",
    "query_embedding = get_embedding(\"What is MongoDB Atlas Search?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where\n",
    "# the `metadata.contentType` field has the value \"Tutorial\"\n",
    "# AND\n",
    "# the `updated` field is greater than or equal to \"2024-05-19\"\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"path\": \"embedding\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"numCandidates\": 150,\n",
    "            \"limit\": 5,\n",
    "            \"filter\": {\n",
    "                \"metadata.contentType\": \"Tutorial\",\n",
    "                \"updated\": {\"$gte\": \"2024-05-19\"}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"body\": 1,\n",
    "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': '# How to Model Your Documents for Vector Search\\n\\nAtlas Vector Search was recently released, so let’s dive into a tutorial on how to properly model your documents when utilizing vector search to revolutionize your querying capabilities!\\n\\n## Data modeling normally in MongoDB\\n\\nVector search is new, so let’s first go over the basic ways of modeling your data in a MongoDB document before continuing on into how to incorporate vector embeddings. \\n\\nData modeling in MongoDB revolves around organizing your data into documents within various collections. Varied projects or organizations will require different ways of structuring data models due to the fact that successful data modeling depends on the specific requirements of each application, and for the most part, no one document design can be applied for every situation. There are some commonalities, though, that can guide the user. These are:',\n",
       "  'score': 0.9525887370109558},\n",
       " {'body': '# How to Deploy MongoDB Atlas with AWS CloudFormation\\n\\nMongoDB Atlas is the multi-cloud developer data platform that provides an integrated suite of cloud database and data services. We help to accelerate and simplify how you build resilient and performant global applications on the cloud provider of your choice.\\n\\nAWS CloudFormation lets you model, provision, and manage AWS and third-party resources like MongoDB Atlas by treating infrastructure as code (IaC). CloudFormation templates are written in either JSON or YAML. \\n\\nWhile there are multiple ways to use CloudFormation to provision and manage your Atlas clusters, such as with Partner Solution Deployments or the AWS CDK, today we’re going to go over how to create your first YAML CloudFormation templates to deploy Atlas clusters with CloudFormation.',\n",
       "  'score': 0.9507526159286499},\n",
       " {'body': 'You will need the following to get started:\\n\\n- An account with Grafbase\\n- An account with MongoDB Atlas\\n- A database with data API access enabled\\n\\n## Enable data API access\\n\\nYou will need a database with MongoDB Atlas to follow along — create one now!\\n\\nFor the purposes of this tutorial, I’ve created a free shared cluster with a single database deployment. We’ll refer to this instance as your “Data Source” later.\\n\\n through the `g.datasource(mongodb)` call.\\n\\n## Create models for data\\n\\nThe MongoDB connector empowers developers to organize their MongoDB collections in a manner that allows Grafbase to autonomously generate the essential queries and mutations for document creation, retrieval, update, and deletion within these collections.\\n\\nWithin Grafbase, each configuration for a collection is referred to as a \"model,\" and you have the flexibility to employ the supported GraphQL Scalars to represent data within the collection(s).',\n",
       "  'score': 0.9448556900024414},\n",
       " {'body': \"To get our MongoDB URI, in our MongoDB Atlas dashboard: \\n\\n 1. Hit the **Connect** button. \\n 2. Then, click the **Connect to your application** button, and here you'll see a string that contains your **URI** that will look like this:\\n\\n```\\nmongodb+srv://:@cluster0..mongodb.net/?retryWrites=true&w=majority\\n```\\nIf you are new to MongoDB Atlas, you'll need to go to the **Database Access** section and create a username and password, as well as the **Network Access** tab to ensure your IP is allowed to connect to the database. However, if you already have a database user and network access enabled, you'll just need to replace the `` and `` fields with your information.\\n\\nFor the ``, we'll load the MongoDB Atlas sample datasets and use one of those databases.\\n\\n, and we'll help troubleshoot.\",\n",
       "  'score': 0.943320631980896},\n",
       " {'body': '## Extras: Querying with vector search\\n\\nWhen you’re ready to query and find results from your embedded documents, it’s time to create an aggregation pipeline on your embedded vector data. To do this, you can use the“$vectorSearch” operator, which is a new aggregation stage in Atlas. It helps execute an Approximate Nearest Neighbor query. \\n\\nFor more information on this step, please check out the tutorial on Developer Center about [building generative AI applications, and our YouTube video on vector search.',\n",
       "  'score': 0.9425933361053467}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the aggregation pipeline and view the results\n",
    "results = collection.aggregate(pipeline)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Build the RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fireworks.client import Fireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Fireworks AI client and the model string\n",
    "fw_client = Fireworks()\n",
    "model = \"accounts/fireworks/models/llama-v3-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create the chat prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the user prompt for our RAG application\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 8\n",
    "    context = vector_search(user_query)\n",
    "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    context = \"\\n\\n\".join([doc.get('body') for doc in context])\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to answer user queries\n",
    "📚 https://docs.fireworks.ai/guides/querying-text-models#chat-completions-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries using Fireworks' Chat Completion API\n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function above to create a chat prompt\n",
    "    prompt = create_prompt(user_query)\n",
    "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
    "    response = fw_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "    # Print the final answer\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas Search is an embedded full-text search in MongoDB Atlas that gives you a seamless, scalable experience for building relevance-based app features.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\"What is MongoDB Atlas Search?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I DON'T KNOW\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🦹‍♀️ Re-rank retrieved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fe30524e7a4f249d461c397443fc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chinsiang\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\chinsiang\\.cache\\huggingface\\hub\\models--mixedbread-ai--mxbai-rerank-xsmall-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a86b031a4642169c7109bb192a3a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cbf8b71a484087a45f9f580b58a37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc33cbe2eba84cecb7cefe983ce798f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d9b5e4abce4fd7abd5ac6e01448105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/8.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac3b3be61ab4d66976368fa77e8aaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7860c36a228b4da2a985c23da6345ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rerank_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a re-ranking step to the following function\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 8\n",
    "    context = vector_search(user_query)\n",
    "    # Extract the \"body\" field from each document in `context`\n",
    "    documents = [d.get(\"body\") for d in context]\n",
    "    # Use the `rerank_model` instantiated above to re-rank `documents`\n",
    "    # Set the `top_k` argument to 5 \n",
    "    reranked_documents = rerank_model.rank(\n",
    "    user_query, documents, return_documents=True, top_k=5\n",
    ")\n",
    "    # Join the re-ranked documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    context = \"\\n\\n\".join([d.get(\"text\", \"\") for d in reranked_documents])\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggers in MongoDB Atlas are used to react to specific operations in a collection, such as insert, update, and delete operations, and automate actions in response to those operations.\n"
     ]
    }
   ],
   "source": [
    "# Note the impact of re-ranking on the generated answer\n",
    "generate_answer(\"What are triggers in MongoDB Atlas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🦹‍♀️ Return streaming responses\n",
    "📚 https://docs.fireworks.ai/guides/querying-text-models#streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries in streaming mode using Fireworks' Chat Completion API \n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function defined in Step 9 to create a chat prompt\n",
    "    prompt = create_prompt(user_query)\n",
    "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
    "    # Set the `stream` parameter to True\n",
    "    response = fw_client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "    # Iterate through the `response` generator and print the results as they are generated\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, MongoDB Atlas Search is an embedded full-text search in MongoDB Atlas that gives you a seamless, scalable experience for building relevance-based app features."
     ]
    }
   ],
   "source": [
    "generate_answer(\"What is MongoDB Atlas Search?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: Add memory to the RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_collection = mongodb_client[DB_NAME][\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'session_id_1'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an index on the key `session_id` for the `history_collection` collection\n",
    "history_collection.create_index(\"session_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to store chat messages in MongoDB\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Store a chat message in a MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID of the message.\n",
    "        role (str): Role for the message. One of `system`, `user` or `assistant`.\n",
    "        content (str): Content of the message.\n",
    "    \"\"\"\n",
    "    # Create a message object with `session_id`, `role`, `content` and `timestamp` fields\n",
    "    # `timestamp` should be set the current timestamp\n",
    "    message = {\n",
    "        \"session_id\": session_id,\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.now(),\n",
    "    }\n",
    "    # Insert the `message` into the `history_collection` collection\n",
    "    history_collection.insert_one(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to retrieve chat history from MongoDB\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.find\n",
    "\n",
    "📚 https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat message history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat message history for.\n",
    "\n",
    "    Returns:\n",
    "        List: List of chat messages.\n",
    "    \"\"\"\n",
    "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
    "    # Sort the results in increasing order of the values in `timestamp` field\n",
    "    cursor =  history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
    "\n",
    "    if cursor:\n",
    "        # Iterate through the cursor and extract the `role` and `content` field from each entry\n",
    "        # Then format each entry as: {\"role\": <role_value>, \"content\": <content_value>}\n",
    "        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in cursor]\n",
    "    else:\n",
    "        # If cursor is empty, return an empty list\n",
    "        messages = []\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle chat history in the generate_answer function\n",
    "📚 https://docs.fireworks.ai/guides/querying-text-models#chat-completions-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(session_id: str, user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user's query taking chat history into account.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat history for.\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Initialize list of messages to pass to the chat completion model\n",
    "    messages = []\n",
    "\n",
    "    # Retrieve documents relevant to the user query and convert them to a single string\n",
    "    context = vector_search(user_query)\n",
    "    context = \"\\n\\n\".join([d.get(\"body\", \"\") for d in context])\n",
    "    # Create a system prompt containing the retrieved context\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\",\n",
    "    }\n",
    "    # Append the system prompt to the `messages` list\n",
    "    messages.append(system_message)\n",
    "    # print(messages)\n",
    "\n",
    "    # Use the `retrieve_session_history` function to retrieve message history from MongoDB for the session ID `session_id` \n",
    "    # And add all messages in the message history to the `messages` list \n",
    "    message_history = retrieve_session_history(session_id)\n",
    "    messages.extend(message_history)\n",
    "    # print(\"after retrieve session history\")\n",
    "    # print(messages)\n",
    "\n",
    "    # Format the user message in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # And append the user message to the `messages` list\n",
    "    user_message = {\"role\": \"user\", \"content\": user_query}\n",
    "    messages.append(user_message)\n",
    "\n",
    "    # Call the chat completions API \n",
    "    response = fw_client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "    # Extract the answer from the API response\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Use the `store_chat_message` function to store the user message and also the generated answer in the message history collection\n",
    "    # The role value for user messages is \"user\", and \"assistant\" for the generated answer\n",
    "    store_chat_message(session_id, \"user\", user_query)\n",
    "    store_chat_message(session_id, \"assistant\", answer)\n",
    "\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Answer the question based only on the following context. If the context is empty, say I DON\\'T KNOW\\n\\nContext:\\n# How to Send MongoDB Document Changes to a Slack Channel\\n\\nIn this tutorial, we will explore a seamless integration of your database with Slack using Atlas Triggers and the Slack API. Discover how to effortlessly send notifications to your desired Slack channels, effectively connecting the operations happening within your collections and relaying them in real-time updates. \\n\\nThe overall flow will be: \\n\\n.\\n\\nOnce this has been completed, we are ready to start creating our first database trigger that will react every time there is an operation in a certain collection. \\n\\n## Atlas trigger\\n\\nFor this tutorial, we will create a trigger that monitors all changes in a `test` collection for `insert`, `update`, and `delete` operations.\\n\\nTo create a new database trigger, you will need to:\\n\\nTo create a new database trigger, you will need to:\\n\\n1. Click the **Data Services** tab in the top navigation of your screen if you haven\\'t already navigated to Atlas.\\n2. Click **Triggers** in the left-hand navigation.\\n3. On the **Overview** tab of the **Triggers** page, click **Add Trigger** to open the trigger configuration page.\\n4. Enter the configuration values for the trigger and click **Save** at the bottom of the page.\\n\\nPlease note that this trigger will make use of the *event ordering* as we want the operations to be processed according to when they were performed. \\n\\nThe trigger configuration values will look like this: \\n\\n using the UI, we need to:   \\n\\n1. Click the **Data Services** tab in the top navigation of your screen if you haven\\'t already navigated to Atlas.\\n\\n2. Click **Functions** in the left navigation menu.\\n\\n## Conclusion\\n\\nI hope you like my template, and I hope I will help you be more productive with MongoDB and the Java stack.\\n\\nIf you see something which can be improved, please feel free to open a GitHub issue or directly submit a pull request. They are very welcome. :-)\\n\\nIf you are new to MongoDB Atlas, give our Quick Start post a try to get up to speed with MongoDB Atlas in no time.\\n\\n  [1]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt876f3404c57aa244/65388189377588ba166497b0/swaggerui.png\\n  [2]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltf2f06ba5af19464d/65388188d31953242b0dbc6f/nyancat.png\\n\\nto only detect certain changes and then adapt the change event to only receive certain fields with a \"$project\".\\n\\n## Conclusion\\n\\nIn this tutorial, we\\'ve learned how to seamlessly integrate your database with Slack using Atlas Triggers and the Slack API. This integration allows you to send real-time notifications to your Slack channels, keeping your team informed about important operations within your database collections.\\n\\nWe started by creating a new application in Atlas and then set up a database trigger that reacts to specific collection operations. We explored the `processEvent` function, which processes change events and prepares the data for Slack notifications. Through a step-by-step process, we demonstrated how to create a message and use the Slack API to post it to a specific channel.\\n\\nNow that you\\'ve grasped the basics, it\\'s time to take your integration skills to the next level. Here are some steps you can follow:\\n\\nThese pre-made templates directly leverage MongoDB Atlas resources from the CloudFormation Public Registry and execute via the AWS CLI/AWS Management Console. Using these is best for users who seek to be tightly integrated into AWS with fine-grained access controls. \\n\\nLet’s get started! \\n\\n*Prerequisites:* \\n\\n- Install and configure an AWS Account and the AWS CLI.\\n- Install and configure the MongoDB Atlas CLI (optional but recommended). \\n\\n## Step 1: Create a MongoDB Atlas account\\n\\nSign up for a free MongoDB Atlas account, verify your email address, and log into your new account.\\n\\nAlready have an AWS account? Atlas supports paying for usage via the AWS Marketplace (AWS MP) without any upfront commitment — simply sign up for MongoDB Atlas via AWS Marketplace.\\n\\nand contact AWS support directly, who can help confirm the CIDR range to be used in your Atlas PAK IP Whitelist. \\n\\n on MongoDB Atlas.'}]\n",
      "I DON'T KNOW\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What are triggers in MongoDB Atlas?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Answer the question based only on the following context. If the context is empty, say I DON\\'T KNOW\\n\\nContext:\\nAccess the complete report, which includes the queries you need - here.\\n\\n## Why MongoDB?\\n    \\nMy lecturer Noel Tierney, a lecturer in Computer Applications in Athlone Institute of Technology, Ireland, gave me the assignment to use MongoDB. He gave them instructions on the project and what kind of outcome he would like to see. I was asked to use MongoDB, and I decided to dive deeper into everything the platform offers. Besides that, as I mentioned briefly in the introduction:  the organization DWF was planning on scaling and expanding their business, and they wanted to look into database development with MongoDB. This was a good chance for me to learn everything about NoSQL.\\n\\n\"listing_id\": \"664017\",\\n \"reviewer_id\": \"3932440\", \\n\"reviewer_name\": \"Carolina\", \\n\"comments\": \"Es una muy buena anfitriona, preocupada de que te encuentres cómoda y te sugiere que actividades puedes realizar. Disfruté mucho la estancia durante esos días, el sector es central y seguro.\" }, ... ]\\n```\\n\\n[1]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt8fcfb82094f04d75/653816cde299fbd2960a4695/image2.png\\n  [2]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc7874f54dc0cd8be/653816e70d850608a2f05bb9/image3.png\\n  [3]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt99aaf337d37c41ae/653816fd2c35813636b3a54d/image1.png\\n\\n[10]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltbff284987187ce16/6504a78bb8c6d6c2d90e6e22/image10.png\\n  [11]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt0ae450069b31dff9/6504a7b99bf261fdd46bddcf/image3.png\\n  [12]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt7f24645eefdab69c/6504a7da9aba461d6e9a55f4/image2.png'}]\n",
      "You asked me \"What are triggers in MongoDB Atlas?\"\n",
      "\n",
      "The answer provided is: I DON'T KNOW\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What did I just ask you? and what is the answer provided?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'Answer the question based only on the following context. If the context is empty, say I DON\\'T KNOW\\n\\nContext:\\nThe Dog database uses fictitious data that I have created myself.  The different practical stages of the project have been documented in my project report and may guide the beginners taking their first steps into MongoDB. \\n\\n## Inspiration\\n\\nThe assignment was given to me by my lecturer. And when he was deciding on the topics for the project, he knew that I love dogs. And that\\'s why my project was all about the dogs.  Even though the lecturer gave me the assignment,  it was my idea to prepare this project in a way that does not only benefit me.\\n\\n## Why MongoDB?\\n    \\nMy lecturer Noel Tierney, a lecturer in Computer Applications in Athlone Institute of Technology, Ireland, gave me the assignment to use MongoDB. He gave them instructions on the project and what kind of outcome he would like to see. I was asked to use MongoDB, and I decided to dive deeper into everything the platform offers. Besides that, as I mentioned briefly in the introduction:  the organization DWF was planning on scaling and expanding their business, and they wanted to look into database development with MongoDB. This was a good chance for me to learn everything about NoSQL.\\n\\n\"listing_id\": \"664017\",\\n \"reviewer_id\": \"3932440\", \\n\"reviewer_name\": \"Carolina\", \\n\"comments\": \"Es una muy buena anfitriona, preocupada de que te encuentres cómoda y te sugiere que actividades puedes realizar. Disfruté mucho la estancia durante esos días, el sector es central y seguro.\" }, ... ]\\n```\\n\\ninterface MoviesProps {\\n   movies: Movie];\\n}\\n\\nconst Movies: React.FC = ({ movies }) => {\\n   return (\\n       \\n\\n           \\n\\nTOP 20 MOVIES OF ALL TIME\\n\\n           \\n\\n               (According to Metacritic)\\n           \\n\\n           \\n\\n               {movies.map((movie) => (\\n                   \\n\\n                       \\n\\n{MOVIE.TITLE}\\n\\n                       \\n\\n{MOVIE.METACRITIC}\\n\\n                       \\n\\n{movie.plot}\\n\\n                   \\n               ))}\\n           \\n\\n       \\n\\n   );\\n};\\n\\nexport default Movies;\\n\\n[1]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt8fcfb82094f04d75/653816cde299fbd2960a4695/image2.png\\n  [2]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/bltc7874f54dc0cd8be/653816e70d850608a2f05bb9/image3.png\\n  [3]: https://images.contentstack.io/v3/assets/blt39790b633ee0d5a7/blt99aaf337d37c41ae/653816fd2c35813636b3a54d/image1.png'}]\n",
      "I apologize for the earlier mistake. Based on the context, I should have said \"I DON'T KNOW\" because triggers were not mentioned in the context. The context only mentioned the Dog database, project report, and MongoDB.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"So what is the answer you gave me based on what you said?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
